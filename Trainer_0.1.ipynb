{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wm3T6JYOhvg",
        "outputId": "054a4950-0072-4879-fcf7-a072f0d37b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ܘܡܠܟܐ ܕܘܝܕ ܣܐܒ ܘܥܠ ܒܫܢܝܐ ܘܡܟܣܝܢ ܗܘܘ ܠܗ ܒܠܒܘܫܐ ܘܠܐ ܫܚ\n",
            "[220, 152, 220, 161, 220, 160, 220, 159, 220, 144, 32, 220, 149, 220, 152, 220, 157, 220, 149, 32, 220, 163, 220, 144, 220, 146, 32, 220, 152, 220, 165, 220, 160, 32, 220, 146, 220, 171, 220, 162, 220, 157, 220, 144, 32, 220, 152, 220, 161, 220, 159, 220]\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "# Construct the full path to the file\n",
        "file_path = os.path.join(current_directory, \"Training Set\", \"result.txt\")\n",
        "\n",
        "# Open the file and read its contents\n",
        "with open(file_path, \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "print (text[:52])\n",
        "print (tokens[:52])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ܘ', 'ܡܠܟܐ', ' ܕܘܝܕ', ' ܣܐܒ', ' ܘ', 'ܥܠ', ' ܒܫܢܝܐ', ' ܘ', 'ܡܟܣܝܢ', ' ܗܘܘ', ' ܠܗ', ' ܒܠܒܘܫܐ', ' ܘ', 'ܠܐ', ' ܫܚܢ', ' ܘ', 'ܐܡܪܘ', ' ܠܗ', ' ܥܒܕܘܗܝ', ' ܗܐ', ' ܥܒܕܝܟ', ' ܩܕܡܝܟ', ' ܢܒܥܘܢ', ' ܠܡܪܢ', ' ܡܠܟܐ', ' ܥܠܝܡܬܐ', ' ܒܬܘܠܬܐ', ' ܘ', 'ܬܩܘܡ', ' ܩܕܡ', ' ܡܠܟܐ', ' ܘ', 'ܬܗܘܐ', ' ܠܗ', ' ܡܫܡܫܢܝܬܐ', ' ܘ', 'ܬܫܟܒ', ' ܒܥܘܒܟ', ' ܘ', 'ܢܫܚܢ', ' ܠܡܪܢ', ' ܡܠܟܐ', ' ܘ', 'ܒܥܘ', ' ܥܠܝܡܬܐ', ' ܕܫܦܝܪܐ', ' ܒܟܠܗ', ' ܬܚܘܡܐ', ' ܕܐܝܣܪܝܠ', ' ܘ', 'ܐܫܟܚܘ', ' ܠܐܒܝܫܓ']\n",
            "<class 'list'>\n",
            "[[220, 152], [220, 161, 220, 160, 220, 159, 220, 144], [32, 220, 149, 220, 152, 220, 157, 220, 149]]\n"
          ]
        }
      ],
      "source": [
        "import regex as re\n",
        "pattern = re.compile(r\"\"\" ?ܘ(?=\\p{L}+)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "# This regex pattern is used to tokenize the text into words and other meaningful units\n",
        "# Here's a breakdown of the pattern:\n",
        "\n",
        "# ܘ(?=\\p{L}+)     : Matches 'ܘ' (Syriac Waw) only if followed by one or more letters\n",
        "#                   This is likely for a specific linguistic feature in Syriac\n",
        "\n",
        "# \\p{L}+          : Matches one or more Unicode letters\n",
        "\n",
        "# \\p{N}+          : Matches one or more Unicode numbers\n",
        "\n",
        "# [^\\s\\p{L}\\p{N}]+: Matches one or more characters that are not whitespace, letters, or numbers\n",
        "\n",
        "# \\s+(?!\\S)       : Matches one or more whitespace characters not followed by a non-whitespace character\n",
        "#                   This is likely to catch trailing spaces\n",
        "\n",
        "# \\s+             : Matches one or more whitespace characters\n",
        "\n",
        "# Each part of the pattern is preceded by a space and a question mark ( ?)\n",
        "# This makes the leading space optional for each token\n",
        "\n",
        "# The pipe symbol (|) separates each part of the pattern, allowing any of these to match\n",
        "\n",
        "\n",
        "\n",
        "text_words = re.findall(pattern, text) # devide the text into words according to gpt2pat pattern\n",
        "print(text_words[:52])\n",
        "print(type(text_words))\n",
        "words_tokens = []\n",
        "for i in text_words:\n",
        "    token_word = i.encode(\"utf-8\")\n",
        "    token = list(map(int, token_word))\n",
        "    words_tokens.append(token)\n",
        "print(words_tokens[:3])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPEjcKJ3Ohvh",
        "outputId": "e009b149-7b93-4bdd-fc5f-2c18e995fa19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(220, 144) 44750\n"
          ]
        }
      ],
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for id in ids:    \n",
        "        for pair in zip(id, id[1:]): # Pythonic way to iterate consecutive elements\n",
        "            if pair[1] != 220:  # Check to avoid pairs ending with 220\n",
        "                counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "\n",
        "def simple_get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        if pair[1] != ord(' ') and pair[1] != 220:  # Check to avoid pairs ending with 220\n",
        "            counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "stats = get_stats(words_tokens)\n",
        "top_pair = max(stats, key=stats.get)\n",
        "print (top_pair, stats[top_pair])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUV5pr3iOhvi",
        "outputId": "10af5b63-b47f-4daf-d01b-bb1dc4451e43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length: 751752\n",
            "[[220, 152], [220, 161, 220, 160, 220, 159, 256], [32, 220, 149, 220, 152, 220, 157, 220, 149], [32, 220, 163, 256, 220, 146], [32, 220, 152], [220, 165, 220, 160], [32, 220, 146, 220, 171, 220, 162, 220, 157, 256], [32, 220, 152], [220, 161, 220, 159, 220, 163, 220, 157, 220, 162], [32, 220, 151, 220, 152, 220, 152], [32, 220, 160, 220, 151], [32, 220, 146, 220, 160, 220, 146, 220, 152, 220, 171, 256], [32, 220, 152], [220, 160, 256], [32, 220, 171, 220, 154, 220, 162], [32, 220, 152], [256, 220, 161, 220, 170, 220, 152], [32, 220, 160, 220, 151], [32, 220, 165, 220, 146, 220, 149, 220, 152, 220, 151, 220, 157], [32, 220, 151, 256], [32, 220, 165, 220, 146, 220, 149, 220, 157, 220, 159], [32, 220, 169, 220, 149, 220, 161, 220, 157, 220, 159], [32, 220, 162, 220, 146, 220, 165, 220, 152, 220, 162], [32, 220, 160, 220, 161, 220, 170, 220, 162], [32, 220, 161, 220, 160, 220, 159, 256], [32, 220, 165, 220, 160, 220, 157, 220, 161, 220, 172, 256], [32, 220, 146, 220, 172, 220, 152, 220, 160, 220, 172, 256], [32, 220, 152], [220, 172, 220, 169, 220, 152, 220, 161], [32, 220, 169, 220, 149, 220, 161], [32, 220, 161, 220, 160, 220, 159, 256], [32, 220, 152], [220, 172, 220, 151, 220, 152, 256], [32, 220, 160, 220, 151], [32, 220, 161, 220, 171, 220, 161, 220, 171, 220, 162, 220, 157, 220, 172, 256], [32, 220, 152], [220, 172, 220, 171, 220, 159, 220, 146], [32, 220, 146, 220, 165, 220, 152, 220, 146, 220, 159], [32, 220, 152], [220, 162, 220, 171, 220, 154, 220, 162], [32, 220, 160, 220, 161, 220, 170, 220, 162], [32, 220, 161, 220, 160, 220, 159, 256], [32, 220, 152], [220, 146, 220, 165, 220, 152], [32, 220, 165, 220, 160, 220, 157, 220, 161, 220, 172, 256], [32, 220, 149, 220, 171, 220, 166, 220, 157, 220, 170, 256], [32, 220, 146, 220, 159, 220, 160, 220, 151], [32, 220, 172, 220, 154, 220, 152, 220, 161, 256], [32, 220, 149, 256, 220, 157, 220, 163, 220, 170, 220, 157, 220, 160], [32, 220, 152], [256, 220, 171, 220, 159, 220, 154, 220, 152], [32, 220, 160, 256, 220, 146, 220, 157, 220, 171, 220, 147]]\n",
            "length: 707002\n"
          ]
        }
      ],
      "source": [
        "def merge(ids, pair, idx):\n",
        "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
        "  newids = []\n",
        "  for sublist in ids:\n",
        "        i = 0\n",
        "        new_sublist = []\n",
        "        while i < len(sublist):\n",
        "            # if we are not at the very last position AND the pair matches, replace it\n",
        "            if i < len(sublist) - 1 and sublist[i] == pair[0] and sublist[i + 1] == pair[1]:\n",
        "                new_sublist.append(idx)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_sublist.append(sublist[i])\n",
        "                i += 1\n",
        "        newids.append(new_sublist)\n",
        "  return newids\n",
        "\n",
        "def simple_merge(ids, pair, idx):\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "def length(ids):\n",
        "    return sum(len(id) for id in ids)\n",
        "\n",
        "print(\"length:\", length(words_tokens))\n",
        "tokens2 = merge(words_tokens, top_pair, 256)\n",
        "print(tokens2[:52])\n",
        "print(\"length:\", length(tokens2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiuP-jzlOhvk",
        "outputId": "58527828-df3f-4a7d-ff5c-3b4d5c60264e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ---\n",
        "vocab_size = 6000 # the desired final vocabulary size\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(words_tokens) # copy so we don't destroy the original list\n",
        "\n",
        "merges = {} # (int, int) -> int\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "  ids = merge(ids, pair, idx)\n",
        "  merges[pair] = idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JawSOPc2Ohvk",
        "outputId": "28f8d2ed-d8f0-4dff-9f01-b2885d398c92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens length: 751752\n",
            "ids length: 111069\n",
            "compression ratio: 6.77X\n"
          ]
        }
      ],
      "source": [
        "print(\"tokens length:\", length(words_tokens))\n",
        "print(\"ids length:\", length(ids))\n",
        "print(f\"compression ratio: {length(words_tokens) / length(ids):.2f}X\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdP1ZmqBOhvl"
      },
      "source": [
        "### Generate Vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OObizjaFOhvl",
        "outputId": "d154e9ca-fd94-4bb9-f9c2-58d4d0f2084d"
      },
      "outputs": [],
      "source": [
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merges.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "vocab_converted= {idx: vocab[idx].decode(\"utf-8\") for idx in vocab if idx > 255}\n",
        "\n",
        "merges_converted = {str(key): value for key, value in merges.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary and merges saved to /home/zhanchen/Dropbox/Projects/Tokenizer/Learning Set/result.txt\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "file_path_voc = os.path.join(current_directory, f\"vocabulary_{vocab_size}.json\")\n",
        "file_path_merge = os.path.join(current_directory, f\"merges_{vocab_size}.json\")\n",
        "\n",
        "\n",
        "with open(file_path_voc, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(vocab_converted, file, indent=4, ensure_ascii=False)\n",
        "with open(file_path_merge, \"w\") as file:\n",
        "    json.dump(merges_converted, file, indent=4)   \n",
        "\n",
        "    \n",
        "print(f\"Vocabulary and merges saved to {file_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

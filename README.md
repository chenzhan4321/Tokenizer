# BPE分词器

这是一个专门为低资源语言设计的BPE（字节对编码）分词模型训练工具。本项目包含了训练模型和进行编码解码操作的完整代码实现。

## 目录

- [项目描述](#项目描述)
- [项目文件](#项目文件)
- [安装说明](#安装说明)
- [使用方法](#使用方法)
- [词汇表和合并规则](#词汇表和合并规则)
- [贡献指南](#贡献指南)
- [许可证](#许可证)

## 项目描述

### 概述

这是一个简单的BPE分词器实现，使用Python开发，专门用于处理低资源语言的分词需求。本项目提供了完整的训练流程和使用方法。

### 特点

- **数据处理**：包含完整的文本预处理和处理工具，包括分词和字节对编码（BPE）
- **代码实现**：提供了训练器（trainer.py）和分词器（tokenizer.py）两个主要模块
- **示例文件**：包含示例数据和完整的使用说明

### 环境要求

- Python 3.6+
- regex库
- json库

## 项目文件

- `trainer.py`: BPE模型训练程序
- `tokenizer.py`: 分词程序
- `vocabulary_{vocab_size}.json`: 词汇表文件
- `merges_{vocab_size}.json`: 合并规则文件

## 安装说明

1. 克隆仓库：
   ```bash
   git clone https://github.com/chenzhan4321/Tokenizer.git
   ```

2. 安装依赖：
   ```bash
   pip install regex
   ```

## 使用方法

### 训练模型

1. 准备训练数据，放入 `Training Set` 文件夹
2. 运行训练程序：
   ```bash
   python trainer.py
   ```
3. 训练完成后会生成词汇表和合并规则文件

### 使用分词器

1. 准备需要分词的文本文件
2. 运行分词程序：
   ```bash
   python tokenizer.py
   ```
3. 查看分词结果

## 词汇表和合并规则

### 词汇表（vocabulary_{vocab_size}.json）
- 包含所有基础字符和BPE合并后的子词单元
- 格式：`{token_id: token_text}`
- 示例：
  ```json
  {
    "32": " ",
    "256": "ܫܠܡ",
    "257": "ܒܝܬ"
  }
  ```

### 合并规则（merges_{vocab_size}.json）
- 记录BPE训练过程中的合并操作
- 格式：`{(token1, token2): new_token_id}`
- 示例：
  ```json
  {
    "(220, 171)": 256,
    "(220, 160)": 257
  }
  ```

## 特别说明

1. 空格作为特殊token（ASCII 32）在整个过程中被保留
2. 支持叙利亚文等特殊语言的处理
3. 提供了详细的分词示例和结果展示

## 贡献指南

欢迎提交问题和改进建议！请通过以下方式参与：
1. 提交Issue报告问题
2. 提交Pull Request贡献代码

## 许可证

本项目采用 MIT 许可证，详见 [LICENSE](LICENSE) 文件。
